# 100 Days of GPU Programming

A daily log of my journey learning and implementing deep learning and parallel computing concepts using CUDA (NVIDIA) and HIP/RoCm (AMD).

---

## 1Ô∏è‚É£ Day 01

Learned how to add two 1D vectors using a basic CUDA kernel.

**Keywords and Variables:**
- `__global__`: Defines a function (kernel) that runs on the GPU, launched with `<<<...>>>`.
- `blockIdx.x`: Current block index within a grid.
- `blockDim.x`: Number of threads per block.
- `threadIdx.x`: Current thread index within the block.

**Memory Management:**
- `cudaMalloc`: Allocates memory on the GPU.
- `cudaMemcpy`: Copies memory between host (CPU) and device (GPU).
- `cudaFree`: Frees allocated GPU memory.

---

## 2Ô∏è‚É£ Day 02

Learned to add two 2D matrices.

**Keywords and Variables:**
- `dim3`: CUDA type for specifying 1D, 2D, or 3D dimensions for grids and blocks.
- `cudaDeviceSynchronize`: Forces CPU to wait for GPU to finish.

**Matrix Addition Formula:**
```
c[i * N + j] = a[i * N + j] + b[i * N + j]
where:
  i: row
  j: column
  N: matrix width
```
Other concepts are similar to Day 01.

---

## 3Ô∏è‚É£ Day 03

Learned to multiply a 2D matrix with a 1D vector.

**Example:**
```
2D matrix:
1 1 1
1 1 1
1 1 1

1D vector:
2 2 2

Result:
6 6 6
```
No new concepts today.

---

## 4Ô∏è‚É£ Day 04

Explored shared memory in CUDA.

**Keywords and Variables:**
- `__shared__`: Declares shared memory accessible by threads in a block.
- `__syncthreads()`: Synchronizes all threads in a block.

**Notes:**
- Static shared memory: `__shared__ int var1[10];`
- Dynamic shared memory: `extern __shared__ int var1[];`

**Partial Sum Example:**
```
sharedMemory[0] = in[0] + in[8] = 1 + 9 = 10
...
sharedMemory[7] = in[7] + in[15] = 8 + 16 = 24
```
Partial sums are then accumulated.

---

## 5Ô∏è‚É£ Day 05

Implemented Layer Normalization in CUDA.

**Formula:**
$$
\text{LayerNorm}(x_i) = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta
$$
Where:
- $\mu$: Mean of $x$
- $\sigma^2$: Variance of $x$
- $\epsilon$: Small constant
- $\gamma$, $\beta$: Learnable parameters

---

## 6Ô∏è‚É£ Day 06

Learned to transpose a matrix in CUDA.

**Keywords:**
- `cudaError_t`: CUDA error code type.
- `cudaGetLastError()`: Retrieves the last CUDA error.

---

## 7Ô∏è‚É£ Day 07

Learned about tiled convolution in CUDA.

**References:**
- [YouTube: Tiled Convolution](https://www.youtube.com/watch?v=ysBrzOTMZlQ)
- [Lecture PDF](https://www.cs.ucr.edu/~nael/217-f15/lectures/217-lec8.pdf)

---

## 8Ô∏è‚É£ Day 08

Learned the Brent-Kung algorithm for fast prefix sum.

**Reference:**
- [YouTube: Brent-Kung Prefix Sum](https://www.youtube.com/watch?v=1G4jfLcnI2w&t=88s)

---

## 9Ô∏è‚É£ Day 09

Implemented Flash Attention forward pass using CUDA and shared memory.

---

## üîü Day 10

Implemented Flash Attention for higher-dimensional tensors.

---

## 1Ô∏è‚É£1Ô∏è‚É£ Day 11

Learned about sparse matrices, ELL and COO storage formats, and their real-world importance.

---

## 1Ô∏è‚É£2Ô∏è‚É£ Day 12

Implemented parallel merge sort in CUDA.

---

## 1Ô∏è‚É£3Ô∏è‚É£ Day 13

Implemented parallel BFS and GELU activation using multiple threads.

---

## 1Ô∏è‚É£4Ô∏è‚É£ Day 14

Built a simple neural network with a linear layer.

---

## 1Ô∏è‚É£5Ô∏è‚É£ Day 15

Implemented a CNN from scratch using CUDA kernels.

---

## 1Ô∏è‚É£6Ô∏è‚É£ Day 16

Implemented FHD (Fully-Hybrid Domain) algorithm for non-cartesian MRI reconstruction in CUDA.

---

## 1Ô∏è‚É£7Ô∏è‚É£ Day 17

Learned and implemented FlashAttention-2 (forward and backward pass).

---

## 1Ô∏è‚É£8Ô∏è‚É£ Day 18

Implemented Naive Bayes with shared memory for prior and likelihood.

---

## 1Ô∏è‚É£9Ô∏è‚É£ Day 19

Used cuBLAS API for vector addition and matrix multiplication.

---

## 2Ô∏è‚É£0Ô∏è‚É£ Day 20

Explored cuDNN API for building fully connected networks and using built-in GEMM functions.

---

## 2Ô∏è‚É£1Ô∏è‚É£ Day 21

Implemented RoPE (Rotary Positional Encoding) in CUDA.

---

## 2Ô∏è‚É£2Ô∏è‚É£ Day 22

Implemented EM algorithm for 1D Gaussian vector clustering.

---

## 2Ô∏è‚É£3Ô∏è‚É£ Day 23

Implemented SwiGLU activation on 2D data.

---

## 2Ô∏è‚É£4Ô∏è‚É£ Day 24

Implemented `atomicAdd` in CUDA to count thread IDs.

---

## 2Ô∏è‚É£5Ô∏è‚É£ Day 25

Implemented Monte Carlo Tree Search in CUDA with 1024 parallel simulations.

---

## 2Ô∏è‚É£6Ô∏è‚É£ Day 26

Implemented histogram loss in parallel using shared memory.

---

## 2Ô∏è‚É£7Ô∏è‚É£ Day 27

Implemented mirror descent in CUDA with parallel threads.

---

## 2Ô∏è‚É£8Ô∏è‚É£ Day 28

Built a micrograd-like autograd engine in CUDA with parallel threads.

---

## 2Ô∏è‚É£9Ô∏è‚É£ Day 29

Learned to use CUDA Graphs for fast computation without changing kernels.

---

## 3Ô∏è‚É£0Ô∏è‚É£ Day 30

Implemented and experimented with deep learning operations and parallel computing using HIP for AMD GPUs. The project is organized into three folders:

**DL/** ‚Äî Deep Learning Operations
- `conv_2d.cpp`: HIP-based 2D convolution for CNNs.
- `flash_attention_forward.cpp`: Efficient attention mechanisms.
- `gelu.cpp`: GELU activation function.
- `layer_norm.cpp`: Layer normalization kernel.
- `rope_hip.cpp`: Rotary positional encoding.

**parallel/** ‚Äî Matrix Operations with Parallelism
- `matmul_rocblas.cpp`: Matrix multiplication using rocBLAS.
- `matrix_add.cpp`: Parallel matrix addition.
- `matrix_trans.cpp`: Matrix transpose with shared memory.
- `parallel_merge.cpp`: Data merging with thread-level parallelism.

**simple/** ‚Äî Introductory Parallel Programs
- `partial_sum.cpp`: Basic reduction (sum).
- `prefix_sum.cpp`: Inclusive prefix sum (scan).
- `vec_reocblas.cpp`: Vector ops with rocBLAS.
- `vector_add.cpp`: Parallel vector addition.
- `vector_matrix_mul.cpp`: Vector-matrix multiplication.

---

## 3Ô∏è‚É£1Ô∏è‚É£ Day 31

Implemented Game of Life using shared memory in CUDA.

---

## 3Ô∏è‚É£2Ô∏è‚É£ Day 32

Implemented SGMM in AMD's HIP kernel.

---

## 3Ô∏è‚É£3Ô∏è‚É£ Day 33

Implemented MLP with ReLU (forward and backward).

---

## 3Ô∏è‚É£4Ô∏è‚É£ Day 34

Benchmarked CUDA vs CPU performance.

## 3Ô∏è‚É£5Ô∏è‚É£ Day 35

Ray tracing using CUDA.

---

## 3Ô∏è‚É£6Ô∏è‚É£ Day 36

Implement the Head Diffusion in HIP(AMD).

---

## 3Ô∏è‚É£7Ô∏è‚É£ Day 37

Implement the vector addition in triton.

---

## 3Ô∏è‚É£8Ô∏è‚É£ Day 38

Implement the Matrix Multiplication in triton.


## 3Ô∏è‚É£9Ô∏è‚É£ Day 39

Implement the Softmax in triton

## 4Ô∏è‚É£0Ô∏è‚É£ Day 40

Implement the fused matmul with relu.

## 4Ô∏è‚É£1Ô∏è‚É£ Day 41

Implement the Conv1d in triton.

## 4Ô∏è‚É£2Ô∏è‚É£ Day 42

Implement the Matmul using autotuing in triton.

## 4Ô∏è‚É£3Ô∏è‚É£ Day 43

Implement the leetgpu attention in cuda.

## 4Ô∏è‚É£4Ô∏è‚É£ Day 44

Implement the conv3d in cuda.

## 4Ô∏è‚É£5Ô∏è‚É£ Day 45

Implement the biods from leetGPU in cuda.

## 4Ô∏è‚É£6Ô∏è‚É£ Day 46

Implement the Muon in cuda with libtorch integration.

## 4Ô∏è‚É£7Ô∏è‚É£ Day 47

Performs multi-GPU parallel optimization using a Bee Colony metaheuristic to find the minimum of a simple sum-of-squares function.

## 4Ô∏è‚É£8Ô∏è‚É£ Day 48

Limited Mem BFGS in cuda 
- Reference: https://outoftardis.github.io/daal/daal/algorithms/optimization-solvers/solvers/lbfgs.html